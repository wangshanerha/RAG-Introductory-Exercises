{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e46ce0c",
   "metadata": {},
   "source": [
    "# RAG实践项目\n",
    "## 一：理论学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2358fe2",
   "metadata": {},
   "source": [
    "## 二：环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699788a9",
   "metadata": {},
   "source": [
    "## 三：读取汽车问答数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8387478",
   "metadata": {},
   "source": [
    "读取问答数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3607bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': '“前排座椅通风”的相关内容在第几页？', 'answer': '', 'reference': ''},\n",
       " {'question': '\"关于车辆的儿童安全座椅固定装置，在哪一页可以找到相关内容？\"',\n",
       "  'answer': '',\n",
       "  'reference': ''},\n",
       " {'question': '“打开前机舱盖”的相关信息在第几页？', 'answer': '', 'reference': ''},\n",
       " {'question': '“打开前机舱盖”这个操作在哪一页？', 'answer': '', 'reference': ''},\n",
       " {'question': '“查看行车记录仪视频”这一项内容在第几页？', 'answer': '', 'reference': ''},\n",
       " {'question': '请问Lynk&Co领克汽车的事件数据记录系统（EDR）主要记录哪些信息？',\n",
       "  'answer': '',\n",
       "  'reference': ''},\n",
       " {'question': '问题：事件数据记录系统（EDR）中的数据是否可以被黑客利用进行恶意攻击？',\n",
       "  'answer': '',\n",
       "  'reference': ''},\n",
       " {'question': '问题：在国家环保法要求下，哪些情况下需要对车辆进行报废处理？', 'answer': '', 'reference': ''},\n",
       " {'question': '请问，如果车辆报废后，原车主是否还能使用该车辆的智能互联服务？',\n",
       "  'answer': '',\n",
       "  'reference': ''},\n",
       " {'question': '如何确保用车前的准备工作万无一失？', 'answer': '', 'reference': ''}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pdfplumber\n",
    "\n",
    "questions = json.load(open(\"questions.json\", encoding='utf-8'))\n",
    "questions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75f8f9",
   "metadata": {},
   "source": [
    "读取可复制的pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acf4cc",
   "metadata": {},
   "source": [
    "读取读取第一页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1101ade6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'欢迎\\n感谢您选择了具有优良安全性、舒适性、动力性和经济性的Lynk&Co领克汽车。\\n首次使用前请仔细、完整地阅读本手册内容，将有助于您更好地了解和使用车辆。\\n本手册中的所有资料均为出版时的最新资料，但本公司将对产品进行不断的改进和优化，您所购的车辆可能与本手册中的描述有所不同，请以实际\\n接收的车辆为准。\\n如您有任何问题，或需要预约服务，请拨打电话4006-010101联系我们。您也可以开车前往Lynk&Co领克中心。\\n在抵达之前，请您注意驾车安全。\\n©领克汽车销售有限公司'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pdfplumber.open(\"初赛训练数据集.pdf\")\n",
    "# len(pdf.pages) # 页数\n",
    "pdf.pages[0].extract_text() # 读取第一页内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50467f62",
   "metadata": {},
   "source": [
    "读取所有页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae75d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'page': 'page_1',\n",
       "  'content': '欢迎\\n感谢您选择了具有优良安全性、舒适性、动力性和经济性的Lynk&Co领克汽车。\\n首次使用前请仔细、完整地阅读本手册内容，将有助于您更好地了解和使用车辆。\\n本手册中的所有资料均为出版时的最新资料，但本公司将对产品进行不断的改进和优化，您所购的车辆可能与本手册中的描述有所不同，请以实际\\n接收的车辆为准。\\n如您有任何问题，或需要预约服务，请拨打电话4006-010101联系我们。您也可以开车前往Lynk&Co领克中心。\\n在抵达之前，请您注意驾车安全。\\n©领克汽车销售有限公司'},\n",
       " {'page': 'page_11',\n",
       "  'content': '前言\\n本手册相关的重要信息 为了凸显重点、便于阅读，中央显示屏中的插图突出显示重点部位，\\n领克汽车销售有限公司（下称“Lynk&Co领克”）建议您在首次使用 其他部位做了一些处理，提示您不必关注这些部位。\\n车辆前，认真阅读本手册内容。为了更好地理解本手册中的内容，您\\n需要了解以下所有信息。 此图标表示“请勿这样做”或“请勿让此种情况发生”。\\n提示信息\\n警告！\\n人身伤害 此图标表示“建议这样做”或“可以让此种情况发生”。\\n■ 警告标识提醒您：如未按照该警告内容操作可能会对您或他人造\\n成人身伤害或生命危险。\\n注意！ 如果一幅插图显示多个零件，此图标表示零件位置。\\n车辆损坏风险\\n■ 注意标识提醒您：如未按照该注意事项操作可能会导致车辆损 如果一幅插图包含两个以上动作，此图标表示包含顺序的移动/\\n坏。 动作。\\n如果一幅插图只包含一个动作，此图标表示移动/动作。\\n说明！\\n帮助信息\\n显示文本\\n□ 在这里您可以找到一些帮助提示或有用的详细信息。\\n车辆显示屏上显示的文本或消息的格式与普通文本不同（例如：取\\n消、系统重置、路径编辑等），程序和指示分多个步骤显示（例如：\\n车辆-车辆设置）。\\n图标和插图说明\\n敬告用户\\n本手册中使用的插图和视频剪辑仅是示意图，展示车辆的某些特征或\\n功能，用于辅助说明，以帮助您理解。此外车辆功能也会随着软件版\\n为了您的安全，请您在驾驶车辆时遵守当地的法律法规，并遵循以下\\n本升级或功能开通情况进行更迭，相应的内容及插图可能会和实车存\\n注意事项：\\n在差异，仅供参考，具体情况请以实车为准。\\n11'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_content = []\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    pdf_content.append({\n",
    "        'page': 'page_' + str(page_idx + 1),\n",
    "        'content': pdf.pages[page_idx].extract_text()\n",
    "    })\n",
    "pdf_content[0],pdf_content[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183fd3f",
   "metadata": {},
   "source": [
    "# 四：文本索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8beb9a6",
   "metadata": {},
   "source": [
    "TFIDF（Term Frequency-Inverse Document Frequency）是一种用于信息检索和文本挖掘的常用权重计算方法，\n",
    "\n",
    "旨在衡量一个词项对于一个文档集合中某个文档的重要性。该方法结合了两个方面的信息：词项在文档中的频率（TF）和在整个文档集合中的逆文档频率（IDF）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eda44b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\WANGSH~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.511 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请问 Lynk & Co 领克 汽车 的 事件 数据 记录 系统 （ EDR ） 主要 记录 哪些 信息 ？\n",
      "前言 \n",
      " 本手册 相关 的 重要 信息   为了 凸显 重点 、 便于 阅读 ， 中央 显示屏 中 的 插图 突出 显示 重点部位 ， \n",
      " 领克 汽车 销售 有限公司 （ 下称 “ Lynk & Co 领克 ” ） 建议您 在 首次 使用   其他 部位 做 了 一些 处理 ， 提示 您 不必 关注 这些 部位 。 \n",
      " 车辆 前 ， 认真 阅读 本手册 内容 。 为了 更好 地 理解 本手册 中 的 内容 ， 您 \n",
      " 需要 了解 以下 所有 信息 。   此 图标 表示 “ 请勿 这样 做 ” 或 “ 请勿 让 此种 情况 发生 ” 。 \n",
      " 提示信息 \n",
      " 警告 ！ \n",
      " 人身 伤害   此 图标 表示 “ 建议 这样 做 ” 或 “ 可以 让 此种 情况 发生 ” 。 \n",
      " ■   警告 标识 提醒您 ： 如未 按照 该 警告 内容 操作 可能 会 对 您 或 他 人造 \n",
      " 成 人身 伤害 或 生命危险 。 \n",
      " 注意 ！   如果 一幅 插图 显示 多个 零件 ， 此 图标 表示 零件 位置 。 \n",
      " 车辆 损坏 风险 \n",
      " ■   注意 标识 提醒您 ： 如未 按照 该 注意事项 操作 可能 会 导致 车辆 损   如果 一幅 插图 包含 两个 以上 动作 ， 此 图标 表示 包含 顺序 的 移动 / \n",
      " 坏 。   动作 。 \n",
      " 如果 一幅 插图 只 包含 一个 动作 ， 此 图标 表示 移动 / 动作 。 \n",
      " 说明 ！ \n",
      " 帮助 信息 \n",
      " 显示 文本 \n",
      " □   在 这里 您 可以 找到 一些 帮助 提示 或 有用 的 详细信息 。 \n",
      " 车辆 显示屏 上 显示 的 文本 或 消息 的 格式 与 普通 文本 不同 （ 例如 ： 取 \n",
      " 消 、 系统 重置 、 路径 编辑 等 ） ， 程序 和 指示 分 多个 步骤 显示 （ 例如 ： \n",
      " 车辆 - 车辆 设置 ） 。 \n",
      " 图标 和 插图 说明 \n",
      " 敬告 用户 \n",
      " 本手册 中 使用 的 插图 和 视频剪辑 仅 是 示意图 ， 展示 车辆 的 某些 特征 或 \n",
      " 功能 ， 用于 辅助 说明 ， 以 帮助 您 理解 。 此外 车辆 功能 也 会 随着 软件版 \n",
      " 为了 您 的 安全 ， 请 您 在 驾驶 车辆 时 遵守 当地 的 法律法规 ， 并 遵循 以下 \n",
      " 本 升级 或 功能 开通 情况 进行 更迭 ， 相应 的 内容 及 插图 可能 会 和 实车存 \n",
      " 注意事项 ： \n",
      " 在 差异 ， 仅供参考 ， 具体情况 请以 实车 为准 。 \n",
      " 11\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # 从 sklearn 提取的工具，用于将文本转化为TF-IDF特征向量\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 对提问和PDF内容进行分词\n",
    "# 将 questions 列表中的提问部分提取出来，使用 jieba.lcut() 进行分词后，词语用空格连接起来。每个提问转换为一个分词后的字符串。\n",
    "# 对PDF内容的 content 部分进行同样的分词操作，将PDF内容中的每页文本转为分词后的字符串。\n",
    "question_words = [' '.join(jieba.lcut(x['question'])) for x in questions]\n",
    "pdf_content_words = [' '.join(jieba.lcut(x['content'])) for x in pdf_content]\n",
    "\n",
    "print(question_words[5])\n",
    "print(pdf_content_words[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d853f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取TFIDF\n",
    "tfidf = TfidfVectorizer()  # 初始化TF-IDF向量生成器。\n",
    "tfidf.fit(question_words + pdf_content_words) # 使用提问和PDF内容的分词结果一起训练TF-IDF模型，学习词汇及其权重。\n",
    "\n",
    "question_feat = tfidf.transform(question_words) # 将分词后的提问转化为TF-IDF特征向量。\n",
    "pdf_content_feat = tfidf.transform(pdf_content_words) # 将分词后的PDF内容转化为TF-IDF特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6d1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行归一化\n",
    "question_feat = normalize(question_feat)\n",
    "pdf_content_feat = normalize(pdf_content_feat)  # 对PDF内容的特征向量进行归一化处理。这一步是为了计算相似度时，确保向量之间的大小不影响最终结果。\n",
    "\n",
    "# 检索进行排序\n",
    "for query_idx, feat in enumerate(question_feat):\n",
    "    score = feat @ pdf_content_feat.T\n",
    "    score = score.toarray()[0]\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "# 生成提交结果\n",
    "with open('submit_TFIDF.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132af925",
   "metadata": {},
   "source": [
    "BM25Okapi是BM25算法的一种变体，它在信息检索中用于评估文档与查询之间的相关性。以下是BM25Okapi的原理和打分方式的概述："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60efc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words) # 初始化BM25模型，输入为PDF每页的分词结果（即 pdf_content_words），模型会学习每个词在所有页面中的分布。\n",
    "# 在这一步中，BM25会根据词频和逆文档频率（类似TF-IDF的概念）为每个词计算权重，并准备好对后续的查询进行评分\n",
    "\n",
    "for query_idx in range(len(questions)):\n",
    "    doc_scores = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "    # 使用BM25模型计算当前提问与所有PDF页面的相似度得分。bm25.get_scores() 会为当前提问计算每个PDF页面的BM25得分。\n",
    "    # doc_scores 是一个包含每页得分的数组，数组的长度等于PDF的页数。\n",
    "    max_score_page_idx = doc_scores.argsort()[-1] + 1\n",
    "    # doc_scores.argsort() 会对得分数组从小到大进行排序，返回每个得分对应的索引位置。[-1] 表示取出最后一个，也就是得分最高的页面的索引值。\n",
    "    # +1 的操作是因为页面的索引从1开始，而数组的索引是从0开始的，所以需要加1来匹配实际的页码。\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "with open('submit_bm25.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4309aa1",
   "metadata": {},
   "source": [
    "# 五：语义检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7907285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\python39_nlp\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "D:\\anaconda\\envs\\python39_nlp\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "D:\\anaconda\\envs\\python39_nlp\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('m3e-base')\n",
    "\n",
    "question_sentences = [x['question'] for x in questions]\n",
    "pdf_content_sentences = [x['content'] for x in pdf_content]\n",
    "\n",
    "#  生成嵌入向量\n",
    "question_embeddings = model.encode(question_sentences, normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)\n",
    "    # normalize_embeddings=True：在生成嵌入时，将向量归一化处理（每个向量的L2范数变为1），确保相似度计算时的结果不会受到向量大小的影响。\n",
    "\n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score = feat @ pdf_embeddings.T\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "with open('submit_m3e.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23cdca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2024 10:36:18 - [INFO] -sentence_transformers.SentenceTransformer->>>    Use pytorch device_name: cuda\n",
      "10/16/2024 10:36:18 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: bce-embedding-base_v1\n",
      "D:\\anaconda\\envs\\python39_nlp\\lib\\site-packages\\transformers\\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.90it/s]\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from BCEmbedding import RerankerModel\n",
    "\n",
    "model = SentenceTransformer(\"bce-embedding-base_v1\")\n",
    "\n",
    "question_sentences = [x['question'] for x in questions]\n",
    "pdf_content_sentences = [x['content'] for x in pdf_content]\n",
    "\n",
    "question_embeddings = model.encode(question_sentences, normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)\n",
    "    # normalize_embeddings=True：在生成嵌入时，将向量归一化处理（每个向量的L2范数变为1），确保相似度计算时的结果不会受到向量大小的影响。\n",
    "\n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score = feat @ pdf_embeddings.T\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "with open('submit_bce.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1984b",
   "metadata": {},
   "source": [
    "# 六：文本多路召回与重排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599a6777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2024 10:36:26 - [INFO] -sentence_transformers.SentenceTransformer->>>    Use pytorch device_name: cuda\n",
      "10/16/2024 10:36:26 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: bce-embedding-base_v1\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.63it/s]\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"bce-embedding-base_v1\")\n",
    "question_sentences = [x['question'] for x in questions]\n",
    "pdf_content_sentences = [x['content'] for x in pdf_content]\n",
    "\n",
    "question_embeddings = model.encode(question_sentences, normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score1 = feat @ pdf_embeddings.T\n",
    "    score2 = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "\n",
    "    score = rankdata(score1) + rankdata(score2)\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "with open('submit_bm25+bce.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0d2b2",
   "metadata": {},
   "source": [
    "# 七：文本问答Promopt优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3206069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zhipuai import ZhipuAI\n",
    "# client = ZhipuAI(api_key=\"f037f80bb31908ef8d2159b5e4f17e6d.kDBaHZjpplHWU6pl\") # 填写Key\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"glm-4-flash\",  # 填写需要调用的模型编码\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"作为童话之王，请以始终保持一颗善良的心为主题，写一篇简短的童话故事。故事应能激发孩子们的学习兴趣和想象力，同时帮助他们更好地理解和接受故事中蕴含的道德和价值观\"}\n",
    "#     ],\n",
    "# )\n",
    "# print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01429d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask_glm(content):\n",
    "#     url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "#     headers = {\n",
    "#       'Content-Type': 'application/json',\n",
    "#       'Authorization': generate_token(\"f037f80bb31908ef8d2159b5e4f17e6d\", 1000)\n",
    "#     }\n",
    "\n",
    "#     data = {\n",
    "#         \"model\": \"glm-4-flash\",\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": content}]\n",
    "#     }\n",
    "\n",
    "#     response = requests.post(url, headers=headers, json=data)\n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8bae624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def ask_glm(content):\n",
    "    # 初始化ZhipuAI客户端，填写你的API Key\n",
    "    client = ZhipuAI(api_key=\"f037f80bb31908ef8d2159b5e4f17e6d.kDBaHZjpplHWU6pl\")\n",
    "    \n",
    "    # 调用ZhipuAI的chat.completions接口，传入参数\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-flash\",  # 指定模型名称\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": content}  # 使用传入的content作为对话的用户输入\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # 返回响应中的生成结果\n",
    "    return response.choices[0].message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebee0336",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m你是一个汽车专家，帮我结合给定的资料，回答一个问题。如果问题无法从资料中获得，请输出结合给定的资料，无法回答问题。\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m资料：\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m问题：\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mpdf_content\u001b[49m[max_score_page_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m     questions[query_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     answer = ask_glm(prompt)['choices'][0]['message']['content']\u001b[39;00m\n\u001b[0;32m     10\u001b[0m answer \u001b[38;5;241m=\u001b[39m ask_glm(prompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf_content' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0acd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际KEY，过期时间\n",
    "def generate_token(apikey: str, exp_seconds: int):\n",
    "    try:\n",
    "        id, secret = apikey.split(\".\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"invalid apikey\", e)\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": id,\n",
    "        \"exp\": int(round(time.time() * 1000)) + exp_seconds * 1000,\n",
    "        \"timestamp\": int(round(time.time() * 1000)),\n",
    "    }\n",
    "    return jwt.encode(\n",
    "        payload,\n",
    "        secret,\n",
    "        algorithm=\"HS256\",\n",
    "        headers={\"alg\": \"HS256\", \"sign_type\": \"SIGN\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c92d71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2024 10:36:36 - [INFO] -sentence_transformers.SentenceTransformer->>>    Use pytorch device_name: cuda\n",
      "10/16/2024 10:36:36 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: m3e-base\n",
      "10/16/2024 10:36:37 - [INFO] -sentence_transformers.SentenceTransformer->>>    Use pytorch device_name: cuda\n",
      "10/16/2024 10:36:37 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: m3e-base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SentenceTransformer('m3e-base')\n",
    "# rerank_model = SentenceTransformer('m3e-base')\n",
    "# rerank_model.cuda()\n",
    "# rerank_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7918b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2024 11:05:11 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:14 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:16 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:22 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:23 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:26 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:28 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:33 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:35 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:44 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:46 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:47 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:55 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:56 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:05:58 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:00 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:02 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:04 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:12 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:15 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:20 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:21 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:27 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:29 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:37 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:39 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:42 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:45 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:47 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:06:53 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:02 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:05 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:10 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:17 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:25 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:29 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:33 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:35 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:38 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:40 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:42 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:44 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:47 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:48 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:07:53 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:02 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:07 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:09 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:12 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:13 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:15 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:18 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:23 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:25 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:28 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:30 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:32 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:34 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:37 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:39 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:41 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:43 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/16/2024 11:08:45 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:08:54 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:02 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:06 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:08 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:10 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:13 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:21 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:26 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n",
      "10/16/2024 11:09:29 - [INFO] -httpx->>>    HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words)\n",
    "\n",
    "# for query_idx in range(len(questions)):\n",
    "#     doc_scores = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "#     max_score_page_idxs = doc_scores.argsort()[-3:]\n",
    "\n",
    "#     pairs = []\n",
    "#     for idx in max_score_page_idxs:\n",
    "#         pairs.append([questions[query_idx][\"question\"], pdf_content[idx]['content']])\n",
    "\n",
    "#     inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         inputs = {key: inputs[key].cuda() for key in inputs.keys()}\n",
    "#         scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "#     max_score_page_idx = max_score_page_idxs[scores.cpu().numpy().argmax()]\n",
    "#     questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx + 1)\n",
    "    \n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score1 = feat @ pdf_embeddings.T\n",
    "    score2 = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "\n",
    "    score = rankdata(score1) + rankdata(score2)\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)\n",
    "\n",
    "    prompt = '''你是一个汽车专家，帮我结合给定的资料，回答一个问题。如果问题无法从资料中获得，请输出结合给定的资料，无法回答问题。\n",
    "资料：{0}\n",
    "\n",
    "问题：{1}\n",
    "    '''.format(\n",
    "        pdf_content[max_score_page_idx]['content'],\n",
    "        questions[query_idx][\"question\"]\n",
    "    )\n",
    "#     answer = ask_glm(prompt)['choices'][0]['message']['content']\n",
    "    answer = ask_glm(prompt)\n",
    "    questions[query_idx]['answer'] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "label = json.load(open('汽车知识问答_label.json'))\n",
    "pred = json.load(open('submit_fusion_bge+bm25_rerank_retrieval_glm4.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68550d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(label, pred):\n",
    "    if y['reference'] in x['reference']:\n",
    "        continue\n",
    "\n",
    "    if np.abs(\n",
    "        np.array(\n",
    "            [int(xx.replace('page_', '')) for xx in x['reference'].split(' ')]\n",
    "        ) - int(y['reference'].replace('page_', ''))).max() > 2:\n",
    "        print(x, y)\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python39_nlp]",
   "language": "python",
   "name": "conda-env-python39_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
